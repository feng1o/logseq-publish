- [[Unix网络编程中的五种IO模型]] [[IO模型]] #interview #card
  card-last-interval:: 4
  card-repeats:: 2
  card-ease-factor:: 2.22
  card-next-schedule:: 2023-07-25T05:07:06.683Z
  card-last-reviewed:: 2023-07-21T05:07:06.683Z
  card-last-score:: 3
	- IO 多路复用有三种实现，在介绍select、poll、epoll
	- 背景:: io和cpu、进程、线程、vm、缓存等关系  [[进程线程]] [[进程上下文]]
- [[IO模型]] 阻塞`IO`、非阻塞`IO`、信号驱动`IO`、`IO`多路复用、异步`IO` 五种。   同步`IO`, 异步`IO`, 阻塞`IO`, 非阻塞`IO`  [#](https://zhaostu4.github.io/2019/11/28/%E6%80%BB%E7%BB%93%E7%B3%BB%E5%88%97-I_O%E5%88%86%E7%B1%BB%E5%92%8CI_O%E6%A8%A1%E5%9E%8B/) #interview #card
  card-last-interval:: 4
  card-repeats:: 2
  card-ease-factor:: 2.22
  card-next-schedule:: 2023-07-25T05:08:21.920Z
  card-last-reviewed:: 2023-07-21T05:08:21.920Z
  card-last-score:: 3
	- 实际上同步与异步是针对应用程序与内核的交互而言的
		- 同步过程中进程,触发`IO`操作并**等待(也就是阻塞)或者轮询**的去查看`IO`操作(也就是非阻塞)是否完成。
		- 异步过程中进程触发`IO`操作以后，直接返回，做自己的事情，`IO`交给内核来处理，**完成后内核通知**进程`IO`完成
	- 阻塞与非阻塞更关注的是单个进程的执行状态
	- **同步有阻塞和非阻塞之分，异步没有，异步一定是非阻塞的。**
		- **阻塞、非阻塞、多路`IO`复用，都是同步`IO`，异步必定是非阻塞的，所以不存在异步阻塞和异步非阻塞的说法。**
		- 真正的异步`IO`需要内核的深度参与。换句话说，只有用户线程在操作`IO`的时候根本不去考虑`IO`的执行全部都交给内核去完成，而自己只等待一个完成信号的时候，才是真正的异步`IO`。所以，用一个子线程去轮询、去死循环，或者使用`select`,` poll`,` epool`, 都不是异步
	- #### 结论
		- **同步**：执行一个操作之后，进程触发`IO`操作并等待(也就是我们说的阻塞)或者[[$red]]==轮询的去==查看`IO`操作(也就是我们说的非阻塞)是否完成，等待结果，然后才继续执行后续的操作。
		- **异步**：执行一个操作后，可以去执行其他的操作，然后等待通知再回来执行刚才没执行完的操作。
			- 这种模型和前非阻塞模型区别在于：信号驱动`I/O`是由内核通知我们何时可以[[$red]]==启动==一个`I/O`操作，而异步`I/O`模型是由内核通知我们`I/O`操作[[$red]]==何时完成==
		- **阻塞**：进程给内核传达一个任务之后，一直等待内核处理完成，然后才执行后面的操作。
		- **非阻塞**：进程给内核传达任我后，继续处理后续的操作[[#green]]==，隔断时间再来询问之前的操作是否完成==。这样的过程其实也叫轮询。
- [[io多路复用]]模型 #interview #card
  card-last-interval:: 4
  card-repeats:: 2
  card-ease-factor:: 2.22
  card-next-schedule:: 2023-07-25T05:07:18.729Z
  card-last-reviewed:: 2023-07-21T05:07:18.729Z
  card-last-score:: 3
	- 与多进程和多线程技术相比， **[[$red]]==I/O多路复用技术的最大优势是系统开销小，系统不必创建进程/线程，也不必维护这些进程/线程==** ，从而大大减小了系统的开销。但select，poll，epoll本质上都是同步I/O，因为他们都需要 **在读写事件就绪后自己负责进行读写** ，也就是说这个读写过程是 **阻塞** 的，而 **异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间**
	- select:: 本质上是通过设置或者检查存放fd标志位的数据结构来进行下一步处理
		- 单个进程所打开的FD是有限制的
		- 每次调用 select，都需要把 fd 集合从用户态拷贝到内核态
		- socket轮询效率低
	- poll :: 没有FD限制
	-
	- epoll
		- epoll使用一个 **epfd** （epoll文件描述符）管理多个socket描述符，epoll不限制socket描述符的个数， **将用户空间的socket描述符的事件存放到内核的一个事件表中**
		   
		  ，这样在用户空间和内核空间的copy只需一次。当epoll记录的socket产生就绪的时候，epoll会通过callback的方式来激活这个fd，这样子在epoll_wait便可以收到通知，告知应用层哪个socket就绪了，这种通知的方式是可以直接得到那个socket就绪的，因此相比于
		   select 和 poll ，它不需要遍历socket列表，时间复杂度是O(1)，不会因为记录的socket增多而导致开销变大。
		-
		- epoll支持水平触发和边沿触发两种模式
			- LT模式：即水平出发模式，当epoll_wait检测到socket描述符处于就绪时就通知应用程序，应用程序可以不立即处理它。下次调用epoll_wait时，还会再次产生通知。
			- ET模式：即边缘触发模式，当epoll_wait检测到socket描述符处于就绪时就通知应用程序，应用程序 **必须** 立即处理它。如果不处理，下次调用epoll_wait时，不会再次产生通知。
			  
			  **ET模式在很大程度上减少了epoll事件被重复触发的次数，因此效率要比LT模式高** 。epoll工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。
		- #### epoll为什么更高效 [](https://doc.embedfire.com/linux/imx6/base/zh/latest/system_programing/socket_io.html#id12)
			- event触发方式，0(1)
			- mmap 减少用户态和内核态之间的数据交换，不需要依赖拷贝
			- **只管就绪的socket描述符，而跟socket描述符的总数无关** 。
-