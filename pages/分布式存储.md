- > [[分布式文件系统]]比较热点的关键技术
	- [[纠删码]]Erasure Code技术 [doc](https://blog.csdn.net/bandaoyu/article/details/122967008)  [多副本和纠删码比较](https://blog.csdn.net/qq43748322/article/details/121114894)
		- [[$sub8]]==实际就是加入校验，实现容错、校验等能力。比如通信传输中分段加入校验，丢失也能恢复。[[检错码]]、[[纠错码]]、[[纠删码]]，纠删码不仅具有识别错、纠正错误能力，当错误太多无法纠正时还具备删除无法纠正数据能力。==
		- 相比[[副本replica]]机制，能以更小的冗余获得更好的[[可靠性]]，但是编码方式复杂，计算量大，重构(异常了恢复)也慢
		- 只能容忍数据丢失，篡改无法
		- EC纠删码存在比较严重的写放大，小块数据写影响较大；一般使用与video、备份等io要求不高的场景；在数据库、虚拟化等[[块存储]]一般用三副本机制
		- 理解:: 4+2，比如a+b+c+d=10, a+2b+c+4d=2,c=1,d=2就可以算出a、b； 实际纠删码[[算法]]类似的效果；用M+N表示纠删码的话，以上就是一个4+2的纠删码方案，[[#red]]==数据会被切分成4个相同大小的分片，并通过校验[[算法]]生成2个同样大小的校验分片P和O==。比如32KB的数据会被切成4个8KB的分片条带，再生出2个8KB的分片，总计48KB数据。当6个数据分片生成后，它们会被随机存到6个不同的[节点上](https://blog.csdn.net/qq43748322/article/details/121114894)， [[$green]]==GPFS的条带比如，blocksize=8M，那8+2P实际上是每个条带写入1M，一共写入10M数据到10个条带上。==
		- ByteStore要求写流程需要满条带大小；例如：EC8+2，条带大小16KB 场景下，PFS向ByteStore写入4KB数据，需要在PFS补齐到8 * 16KB；存在32倍写放大；？
		- 读写放大问题:
			- > x+yP里的yP只在写和数据重建用到。读的时候x要是都在，只需要把x读出来就行
			  4+2，客户端发读4M的请求到nsd server，nsd sever到对应的4个nspdserver读4个条带数据。这里有可能某个条带在nsd server本地，也有可能4个条带都不在本地。
			  但无论如何，只要4个条带没丢，就读4个数据条带就行。
			  读，因为过两次网络所以放大倍数一直是2,跟ec code没关系，一次是nsd client到nsd server，一次nsd server到nspd server。因为中间只涉及到数据不涉及到parity，所以放大就是2
			  写的话，client写4m数据到nsd server，nsd server要写4+2个条带到6个nspd server。那这样就是（4+6/4）=2.5倍的放大
			-
	- 一致性协议
	- 数据定位技术
	- 日志技术
	- [[RDMA]]
- [[分布式文件系统]]，可以优先支持客户端文件系统；一般都支持客户端文件系统访问
	- 采用FUSE实现
	- 文件系统挂载方式（直接内核支持)
- [[分布式文件系统]] [doc](https://dbaplus.cn/news-159-3047-1.html)  [[#red]]==基础的文件FAQ== <a class="alg-medium"></a> #card
  card-last-interval:: -1
  card-repeats:: 1
  card-ease-factor:: 2.5
  card-next-schedule:: 2024-01-15T16:00:00.000Z
  card-last-reviewed:: 2024-01-15T11:42:46.108Z
  card-last-score:: 1
	- > 容量大、加速读写效率、保证文件高可用
	- 架构::  组件(存储组件、接口组件、管理组件)；部署上: 有中心、无中心ceph [[$sub8-blue]]==Client 必须要知道，当对某个文件进行操作时，它该访问集群中的哪个节点==
	- 持久化
	  background-color:: firebrick
		- 如何保证每个副本的数据是一致的?  [[$sub8-blue]]==并行写、链式写、CAP==
		  background-color:: olivedrab
		- 如何分散副本，以使灾难发生时，不至于所有副本都被损坏?
		  background-color:: black
		- 怎么检测被损坏或数据过期的副本，以及如何处理?   [[$sub8-blue]]==中心节点的各自上报、无中心的monitor扫==
		  background-color:: cadetblue
		- 该返回哪个副本给 Client?  [[$sub8-blue]]==比如 round-robin、速度最快的节点、成功率最高的节点、CPU 资源最空闲的节点==
		  background-color:: lightseagreen
	- 伸缩性
	  background-color:: peru
		- 如何尽量使各存储节点的负载相对均衡?   [[$sub8-blue]]==存储、cpu、内存来均衡==
		  background-color:: green
		- 怎样保证新加入的节点，不会因短期负载压力过大而崩塌?  [[$sub8]]==预热，防止新加入的资源利用率低大量写入==
		  background-color:: steelblue
		- 如果需要数据迁移，那如何使其对业务层透明?   [[$sub8-blue]]==中心节点的直接干预、非中心的比如ceph采用逻辑和物理结构两层，对外暴露逻辑结构==
		  background-color:: grey
		-
	- 性能优化和缓存一致性
	  background-color:: deeppink
		- 内存中缓存文件内容；
		- 预加载数据块，以避免客户端等待；
		- 合并读写请求，也就是将单次请求做些积累，以批量方式发送给 Server 端。
		- 缓存的使用在提高读写性能的同时，也会带来数据不一致的问题：
			- 会出现更新丢失的现象。当多个 Client 在一个时间段内，先后写入同一个文件时，先写入的 Client 可能会丢失其写入内容，因为可能会被后写入的 Client 的内容覆盖掉；
			- 数据可见性问题。Client 读取的是自己的缓存，在其过期之前，如果别的 Client 更新了文件内容，它是看不到的；也就是说，在同一时间，不同 Client 读取同一个文件，内容可能不一致。
	- [[安全]]性
		- DAC：全称是 Discretionary Access Control，如linux的own、group、user
	- 文件指纹
	  background-color:: purple
- [[GPFS]]
-
-